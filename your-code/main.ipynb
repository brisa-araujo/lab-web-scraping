{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "gitlink = requests.get(url).content\n",
    "git_soup = BeautifulSoup(gitlink, 'html')\n",
    "#gitsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frank Denis',\n",
       " 'Vladimir Mihailenco',\n",
       " 'Henrique Dias',\n",
       " 'Kyle Roach',\n",
       " 'Erik Rasmussen',\n",
       " 'Franck Nijhof',\n",
       " 'Robert Wagner',\n",
       " 'François Beaufort',\n",
       " 'Pascal Birchler',\n",
       " 'Francois Zaninotto',\n",
       " 'Olle Jonsson',\n",
       " 'Samuel Reed',\n",
       " 'Robert Mosolgo',\n",
       " 'Matt (IPv4) Cowley',\n",
       " 'William Durand',\n",
       " 'Felix Rieseberg',\n",
       " 'Felix Angelov',\n",
       " 'Artur Arseniev',\n",
       " 'Michael Skelton',\n",
       " 'Jack Lloyd',\n",
       " 'Federico Brigante',\n",
       " 'R.I.Pienaar',\n",
       " 'Raphaël Benitte',\n",
       " 'Richard Littauer',\n",
       " 'Steven Macenski']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "table = git_soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "#table = [[c for c in e.text.replace('\\xa0','').split('\\n') if c!=''] for e in table]\n",
    "#table\n",
    "\n",
    "names = [item.text for item in table]\n",
    "#names = []\n",
    "#for item in table:\n",
    "    #names.append(item.text)\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frank Denis (jedisct1)',\n",
       " 'Vladimir Mihailenco (vmihailenco)',\n",
       " 'Henrique Dias (hacdias)',\n",
       " 'Kyle Roach (iRoachie)',\n",
       " 'Erik Rasmussen (erikras)',\n",
       " 'Franck Nijhof (frenck)',\n",
       " 'Robert Wagner (rwwagner90)',\n",
       " 'François Beaufort (beaufortfrancois)',\n",
       " 'Pascal Birchler (swissspidy)',\n",
       " 'Francois Zaninotto (fzaninotto)',\n",
       " 'Olle Jonsson (olleolleolle)',\n",
       " 'Samuel Reed (STRML)',\n",
       " 'Robert Mosolgo (rmosolgo)',\n",
       " 'Matt (IPv4) Cowley (MattIPv4)',\n",
       " 'William Durand (willdurand)',\n",
       " 'Felix Rieseberg (felixrieseberg)',\n",
       " 'Felix Angelov (felangel)',\n",
       " 'Artur Arseniev (artf)',\n",
       " 'Michael Skelton (codingo)',\n",
       " 'Jack Lloyd (randombit)',\n",
       " 'Federico Brigante (bfred-it)',\n",
       " 'R.I.Pienaar (ripienaar)',\n",
       " 'Raphaël Benitte (plouc)',\n",
       " 'Richard Littauer (RichardLitt)',\n",
       " 'Steven Macenski (SteveMacenski)']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = git_soup.find_all('p',{'class':'f4 text-normal mb-1'})\n",
    "table2\n",
    "\n",
    "user_name = []\n",
    "for item in table2:\n",
    "    user_name.append(item.text)\n",
    "\n",
    "user_name = [item.replace('\\n','')for item in user_name]\n",
    "dev_list = [f'{names[i]} ({user_name[i]})' for i in range(len(names))]\n",
    "dev_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "git_repos = requests.get(url2).content\n",
    "repos_soup = BeautifulSoup(git_repos,'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gto76</td>\n",
       "      <td>python-cheatsheet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j3ssie</td>\n",
       "      <td>Osmedeus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tangzixiang0304</td>\n",
       "      <td>Shielded_detector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uber</td>\n",
       "      <td>ludwig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xinshuoweng</td>\n",
       "      <td>AB3DMOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NVlabs</td>\n",
       "      <td>stylegan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dagster-io</td>\n",
       "      <td>dagster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensorflow</td>\n",
       "      <td>models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>eragonruan</td>\n",
       "      <td>text-detection-ctpn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sherlock-project</td>\n",
       "      <td>sherlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deepfakes</td>\n",
       "      <td>faceswap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nbei</td>\n",
       "      <td>Deep-Flow-Guided-Video-Inpainting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>iovisor</td>\n",
       "      <td>bcc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Roibal</td>\n",
       "      <td>Cryptocurrency-Trading-Bots-Python-Beginner-A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NVIDIA</td>\n",
       "      <td>DeepLearningExamples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BlackHC</td>\n",
       "      <td>tfpyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clovaai</td>\n",
       "      <td>deep-text-recognition-benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tkat0</td>\n",
       "      <td>PyTorch_BlazeFace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OpenMined</td>\n",
       "      <td>PySyft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CoreyMSchafer</td>\n",
       "      <td>code_snippets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>d2l-ai</td>\n",
       "      <td>d2l-zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>apache</td>\n",
       "      <td>airflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>beecost</td>\n",
       "      <td>bee-university</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sundowndev</td>\n",
       "      <td>PhoneInfoga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user                                               repo\n",
       "0              gto76                                  python-cheatsheet \n",
       "1             j3ssie                                           Osmedeus \n",
       "2    tangzixiang0304                                  Shielded_detector \n",
       "3               uber                                             ludwig \n",
       "4        xinshuoweng                                            AB3DMOT \n",
       "5             NVlabs                                           stylegan \n",
       "6         dagster-io                                            dagster \n",
       "7         tensorflow                                             models \n",
       "8         eragonruan                                text-detection-ctpn \n",
       "9   sherlock-project                                           sherlock \n",
       "10         deepfakes                                           faceswap \n",
       "11              nbei                  Deep-Flow-Guided-Video-Inpainting \n",
       "12           iovisor                                                bcc \n",
       "13            Roibal    Cryptocurrency-Trading-Bots-Python-Beginner-A...\n",
       "14            NVIDIA                               DeepLearningExamples \n",
       "15           BlackHC                                             tfpyth \n",
       "16           clovaai                    deep-text-recognition-benchmark \n",
       "17             tkat0                                  PyTorch_BlazeFace \n",
       "18         OpenMined                                             PySyft \n",
       "19     CoreyMSchafer                                      code_snippets \n",
       "20       public-apis                                        public-apis \n",
       "21            d2l-ai                                             d2l-zh \n",
       "22            apache                                            airflow \n",
       "23           beecost                                     bee-university \n",
       "24        sundowndev                                        PhoneInfoga "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = repos_soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "names2 = [item.text for item in table2]\n",
    "names2 = [item.replace('\\n','')for item in names2]\n",
    "names2 = [item.split('/') for item in names2]\n",
    "best_repos = pd.DataFrame(names2, columns=['user', 'repo'])\n",
    "best_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(url).content\n",
    "disney_soup = BeautifulSoup(disney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "disney_table = disney_soup.find_all('img', {'src':re.compile('.jpg')})\n",
    "\n",
    "images = [item['src'] for item in disney_table]\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(url).content\n",
    "python_soup = BeautifulSoup(python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Pythons_2',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CPython',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_description',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Portal:Contents',\n",
       " '/wiki/Portal:Featured_content',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "python_table = python_soup.find_all(href=re.compile('^/wiki/'))\n",
    "\n",
    "links = [item['href'] for item in python_table]\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "uscode = requests.get(url).content\n",
    "uscode_soup = BeautifulSoup(uscode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title Number</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title 2</td>\n",
       "      <td>The Congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title 6</td>\n",
       "      <td>Domestic Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title 7</td>\n",
       "      <td>Agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title 15</td>\n",
       "      <td>Commerce and Trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title 16</td>\n",
       "      <td>Conservation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Title 19</td>\n",
       "      <td>Customs Duties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Title 21</td>\n",
       "      <td>Food and Drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Title 26</td>\n",
       "      <td>Internal Revenue Code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Title 34</td>\n",
       "      <td>Crime Control and Law Enforcement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Title 38</td>\n",
       "      <td>Veterans' Benefits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Title 42</td>\n",
       "      <td>The Public Health and Welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Title 43</td>\n",
       "      <td>Public Lands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Title 48</td>\n",
       "      <td>Territories and Insular Possessions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Title 49</td>\n",
       "      <td>Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Title 50</td>\n",
       "      <td>War and National Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Title Number                                 Title\n",
       "0      Title 2                           The Congress\n",
       "1      Title 6                      Domestic Security\n",
       "2      Title 7                            Agriculture\n",
       "3     Title 15                     Commerce and Trade\n",
       "4     Title 16                           Conservation\n",
       "5     Title 19                         Customs Duties\n",
       "6     Title 21                         Food and Drugs\n",
       "7     Title 26                  Internal Revenue Code\n",
       "8     Title 34      Crime Control and Law Enforcement\n",
       "9     Title 38                     Veterans' Benefits\n",
       "10    Title 42          The Public Health and Welfare\n",
       "11    Title 43                           Public Lands\n",
       "12    Title 48    Territories and Insular Possessions\n",
       "13    Title 49                         Transportation\n",
       "14    Title 50               War and National Defense"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "uscode_table = uscode_soup.find_all('div',{'class':'usctitlechanged'})\n",
    "\n",
    "uscode_change = [item.text for item in uscode_table]\n",
    "uscode_change = [re.findall(r'(?<=\\b).+(?=\\b)', item) for item in uscode_change]\n",
    "uscode_df = pd.DataFrame([item[0].split('-') for item in uscode_change], columns=['Title Number', 'Title'])\n",
    "uscode_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "fbi = requests.get(url).content\n",
    "fbi_soup = BeautifulSoup(fbi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alejandro Rosales Castillo',\n",
       " 'Yaser Abdel Said',\n",
       " 'Jason Derek Brown',\n",
       " 'Rafael Caro-Quintero',\n",
       " 'Alexis Flores',\n",
       " 'Eugene Palmer',\n",
       " 'Santiago Villalba Mederos',\n",
       " 'Robert William Fisher',\n",
       " 'Bhadreshkumar Chetanbhai Patel',\n",
       " 'Arnoldo Jimenez']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "fbi_table = fbi_soup.find_all('h3',{'class':'title'})\n",
    "fbi_wanted = [item.text.title().replace('\\n','') for item in fbi_table]\n",
    "fbi_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "equake = requests.get(url).content\n",
    "equake_soup = BeautifulSoup(equake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Mag</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>19:42:25.716min ago</td>\n",
       "      <td>35.61 N</td>\n",
       "      <td>117.47 W</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>19:23:50.134min ago</td>\n",
       "      <td>36.19 N</td>\n",
       "      <td>117.89 W</td>\n",
       "      <td>2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>19:16:53.841min ago</td>\n",
       "      <td>38.45 N</td>\n",
       "      <td>16.91 E</td>\n",
       "      <td>30</td>\n",
       "      <td>2.6</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>19:04:00.254min ago</td>\n",
       "      <td>35.96 N</td>\n",
       "      <td>117.71 W</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>19:01:00.857min ago</td>\n",
       "      <td>35.68 N</td>\n",
       "      <td>117.54 W</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>18:50:16.21hr 08min ago</td>\n",
       "      <td>43.62 N</td>\n",
       "      <td>75.40 E</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>EASTERN KAZAKHSTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>18:36:26.81hr 22min ago</td>\n",
       "      <td>35.74 N</td>\n",
       "      <td>117.56 W</td>\n",
       "      <td>8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>18:15:26.51hr 43min ago</td>\n",
       "      <td>28.47 N</td>\n",
       "      <td>56.76 E</td>\n",
       "      <td>10</td>\n",
       "      <td>4.3</td>\n",
       "      <td>SOUTHERN IRAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>17:48:24.02hr 10min ago</td>\n",
       "      <td>9.93 S</td>\n",
       "      <td>118.23 E</td>\n",
       "      <td>10</td>\n",
       "      <td>4.1</td>\n",
       "      <td>REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>17:39:43.02hr 18min ago</td>\n",
       "      <td>0.54 S</td>\n",
       "      <td>127.86 E</td>\n",
       "      <td>10</td>\n",
       "      <td>4.1</td>\n",
       "      <td>HALMAHERA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>17:05:45.02hr 52min ago</td>\n",
       "      <td>36.20 N</td>\n",
       "      <td>117.90 W</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>17:01:30.82hr 57min ago</td>\n",
       "      <td>36.10 N</td>\n",
       "      <td>117.90 W</td>\n",
       "      <td>4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>16:45:55.03hr 12min ago</td>\n",
       "      <td>18.99 N</td>\n",
       "      <td>70.09 W</td>\n",
       "      <td>6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>16:43:21.73hr 15min ago</td>\n",
       "      <td>36.03 N</td>\n",
       "      <td>117.79 W</td>\n",
       "      <td>1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>16:28:38.13hr 29min ago</td>\n",
       "      <td>35.92 N</td>\n",
       "      <td>117.68 W</td>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>16:26:00.53hr 32min ago</td>\n",
       "      <td>30.57 N</td>\n",
       "      <td>141.98 E</td>\n",
       "      <td>2</td>\n",
       "      <td>IZU</td>\n",
       "      <td>JAPAN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>16:01:04.33hr 57min ago</td>\n",
       "      <td>62.22 N</td>\n",
       "      <td>150.00 W</td>\n",
       "      <td>44</td>\n",
       "      <td>2.3</td>\n",
       "      <td>CENTRAL ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>15:57:04.44hr 01min ago</td>\n",
       "      <td>36.10 N</td>\n",
       "      <td>117.82 W</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>15:26:26.64hr 32min ago</td>\n",
       "      <td>35.68 N</td>\n",
       "      <td>117.51 W</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>15:13:22.14hr 45min ago</td>\n",
       "      <td>35.90 N</td>\n",
       "      <td>117.67 W</td>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>15:04:05.14hr 54min ago</td>\n",
       "      <td>35.59 N</td>\n",
       "      <td>117.42 W</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>14:55:01.95hr 03min ago</td>\n",
       "      <td>38.23 N</td>\n",
       "      <td>40.95 E</td>\n",
       "      <td>4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>14:35:44.05hr 22min ago</td>\n",
       "      <td>18.35 S</td>\n",
       "      <td>120.39 E</td>\n",
       "      <td>10</td>\n",
       "      <td>3.5</td>\n",
       "      <td>WESTERN AUSTRALIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>14:02:10.85hr 56min ago</td>\n",
       "      <td>36.12 N</td>\n",
       "      <td>117.82 W</td>\n",
       "      <td>3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>earthquake2019-07-16</td>\n",
       "      <td>13:55:43.56hr 02min ago</td>\n",
       "      <td>35.96 N</td>\n",
       "      <td>117.28 W</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date                        Time Latitude   Longitude  \\\n",
       "0   earthquake2019-07-16         19:42:25.716min ago  35.61 N    117.47 W   \n",
       "1   earthquake2019-07-16         19:23:50.134min ago  36.19 N    117.89 W   \n",
       "2   earthquake2019-07-16         19:16:53.841min ago  38.45 N    16.91 E    \n",
       "3   earthquake2019-07-16         19:04:00.254min ago  35.96 N    117.71 W   \n",
       "4   earthquake2019-07-16         19:01:00.857min ago  35.68 N    117.54 W   \n",
       "5   earthquake2019-07-16     18:50:16.21hr 08min ago  43.62 N    75.40 E    \n",
       "6   earthquake2019-07-16     18:36:26.81hr 22min ago  35.74 N    117.56 W   \n",
       "7   earthquake2019-07-16     18:15:26.51hr 43min ago  28.47 N    56.76 E    \n",
       "8   earthquake2019-07-16     17:48:24.02hr 10min ago  9.93 S    118.23 E    \n",
       "9   earthquake2019-07-16     17:39:43.02hr 18min ago  0.54 S    127.86 E    \n",
       "10  earthquake2019-07-16     17:05:45.02hr 52min ago  36.20 N    117.90 W   \n",
       "11  earthquake2019-07-16     17:01:30.82hr 57min ago  36.10 N    117.90 W   \n",
       "12  earthquake2019-07-16     16:45:55.03hr 12min ago  18.99 N    70.09 W    \n",
       "13  earthquake2019-07-16     16:43:21.73hr 15min ago  36.03 N    117.79 W   \n",
       "14  earthquake2019-07-16     16:28:38.13hr 29min ago  35.92 N    117.68 W   \n",
       "15  earthquake2019-07-16     16:26:00.53hr 32min ago  30.57 N    141.98 E   \n",
       "16  earthquake2019-07-16     16:01:04.33hr 57min ago  62.22 N    150.00 W   \n",
       "17  earthquake2019-07-16     15:57:04.44hr 01min ago  36.10 N    117.82 W   \n",
       "18  earthquake2019-07-16     15:26:26.64hr 32min ago  35.68 N    117.51 W   \n",
       "19  earthquake2019-07-16     15:13:22.14hr 45min ago  35.90 N    117.67 W   \n",
       "20  earthquake2019-07-16     15:04:05.14hr 54min ago  35.59 N    117.42 W   \n",
       "21  earthquake2019-07-16     14:55:01.95hr 03min ago  38.23 N    40.95 E    \n",
       "22  earthquake2019-07-16     14:35:44.05hr 22min ago  18.35 S    120.39 E   \n",
       "23  earthquake2019-07-16     14:02:10.85hr 56min ago  36.12 N    117.82 W   \n",
       "24  earthquake2019-07-16     13:55:43.56hr 02min ago  35.96 N    117.28 W   \n",
       "\n",
       "   Depth   Mag           Region Name  \n",
       "0      1  2.2    SOUTHERN CALIFORNIA  \n",
       "1      2  2.7     CENTRAL CALIFORNIA  \n",
       "2     30  2.6         SOUTHERN ITALY  \n",
       "3      2  2.0     CENTRAL CALIFORNIA  \n",
       "4      6  2.5    SOUTHERN CALIFORNIA  \n",
       "5      1  3.2     EASTERN KAZAKHSTAN  \n",
       "6      8  2.7    SOUTHERN CALIFORNIA  \n",
       "7     10  4.3          SOUTHERN IRAN  \n",
       "8    10   4.1      REGION, INDONESIA  \n",
       "9    10   4.1   HALMAHERA, INDONESIA  \n",
       "10     1  2.9     CENTRAL CALIFORNIA  \n",
       "11     4  2.1     CENTRAL CALIFORNIA  \n",
       "12    6   2.9     DOMINICAN REPUBLIC  \n",
       "13     1  2.4     CENTRAL CALIFORNIA  \n",
       "14     2  2.3     CENTRAL CALIFORNIA  \n",
       "15     2  IZU           JAPAN REGION  \n",
       "16    44  2.3         CENTRAL ALASKA  \n",
       "17     2  3.0     CENTRAL CALIFORNIA  \n",
       "18     3  2.0    SOUTHERN CALIFORNIA  \n",
       "19     2  2.1     CENTRAL CALIFORNIA  \n",
       "20     4  2.0    SOUTHERN CALIFORNIA  \n",
       "21     4  2.1         EASTERN TURKEY  \n",
       "22    10  3.5      WESTERN AUSTRALIA  \n",
       "23     3  2.6     CENTRAL CALIFORNIA  \n",
       "24     0  2.1     CENTRAL CALIFORNIA  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "equake_table = equake_soup.find_all('tr', {'class':'ligne1 normal'})\n",
    "equake_table = [q.text for q in equake_table]\n",
    "equake_almost = pd.Series(equake_table)\n",
    "equake_0 = equake_almost.str.extract(r'(earthquake2019-07-16)')\n",
    "equake_1 = equake_almost.str.extract(r'((?<=earthquake2019-07-16).+ago)')\n",
    "equake_2 = equake_almost.str.extract(r'((?<=ago).{7})')\n",
    "equake_3 = equake_almost.str.extract(r'((?<=ago.{7}).{10})')\n",
    "equake_4 = equake_almost.str.extract(r'((?<=ago.{17}).{1,4}(?=M|m))')\n",
    "equake_5 = equake_almost.str.extract(r'(.{4}(?=[A-Z][A-Z]{5,}))')\n",
    "equake_6 = equake_almost.str.extract(r'([A-Z]+.{1,3}[A-Z]+(?=.{16}$))')\n",
    "equake_total = pd.concat([equake_0, equake_1, equake_2, equake_3, equake_4, equake_5, equake_6], axis=1)\n",
    "equake_total.columns = ['Date', 'Time','Latitude','Longitude','Depth','Mag','Region Name']\n",
    "equake_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, and title of upcoming hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'\n",
    "hack = requests.get(url).content\n",
    "hack_soup = BeautifulSoup(hack, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Geist Hackathon by SefrWahed</td>\n",
       "      <td>7/29/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Code Factor</td>\n",
       "      <td>5/21/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TECHFEST MUNICH</td>\n",
       "      <td>9/6/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galileo App Competition</td>\n",
       "      <td>1/31/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Title       Date\n",
       "0  Code Geist Hackathon by SefrWahed  7/29/2019\n",
       "1                    The Code Factor  5/21/2019\n",
       "2                    TECHFEST MUNICH   9/6/2019\n",
       "3            Galileo App Competition  1/31/2019"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "hack_titles = hack_soup.find_all('h5', {'class':'card-title'})\n",
    "hack_titles = [title.text for title in hack_titles]\n",
    "hack_titles = pd.Series(hack_titles)\n",
    "\n",
    "hack_dates = hack_soup.find_all('p', {'class':'card-text'})\n",
    "hack_d_cl = []\n",
    "for hack in hack_dates:\n",
    "    hack_d_cl.append(hack.text)\n",
    "hack_d_cl = pd.Series(hack_d_cl)\n",
    "hack_d_cl = hack_d_cl.str.extract(r'(.{9}(?=\\n))')\n",
    "hack_d_cl\n",
    "\n",
    "hack_total = pd.concat([hack_titles, hack_d_cl], axis=1)\n",
    "hack_total.columns = ['Title', 'Date']\n",
    "hack_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a twitter user name (without @): brisae567890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'User not found.'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "def no_tweets():\n",
    "    url = 'https://twitter.com/'\n",
    "    user = input('enter a twitter user name (without @):')\n",
    "    page = requests.get(f'{url}{user}/').content\n",
    "    page_soup = BeautifulSoup(page, 'html5lib')\n",
    "    try:\n",
    "        tweets = page_soup.select('.ProfileNav-item:nth-child(1) span:last-child')\n",
    "        tweets = [a.text for a in tweets]\n",
    "        tweets = re.findall(r'.+(?=\\n)', tweets[0])\n",
    "        return f'The number of tweets for user {user} is {tweets}.'\n",
    "    except:\n",
    "        return \"User not found.\"\n",
    "\n",
    "no_tweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a twitter user name (without @): sopitas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The number of followers for user sopitas is ['3,04\\\\xa0M'].\""
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def no_followers():\n",
    "    url = 'https://twitter.com/'\n",
    "    user = input('enter a twitter user name (without @):')\n",
    "    page = requests.get(f'{url}{user}/').content\n",
    "    page_soup = BeautifulSoup(page, 'html5lib')\n",
    "    try:\n",
    "        followers = page_soup.select('.ProfileNav-item:nth-child(3) span:last-child')\n",
    "        followers = [a.text for a in followers]\n",
    "        return f'The number of followers for user {user} is {followers}.'\n",
    "    except:\n",
    "        return \"User not found.\"\n",
    "no_followers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "wiki_url = 'https://www.wikipedia.org/'\n",
    "wiki_link = requests.get(wiki_url).content\n",
    "wiki_soup = BeautifulSoup(wiki_link, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "wiki_table = wiki_soup.find_all('div',{'class':'central-featured'})\n",
    "wiki_info = [item.text for item in wiki_table]\n",
    "\n",
    "wiki_info = [re.findall(r'(?<=\\b).+(?=\\b)', item) for item in wiki_info]\n",
    "#wiki_info = ([re.findall(r'\\','', item) for item in wiki_info])\n",
    "publications = []\n",
    "languajes = []\n",
    "wiki_info = wiki_info[0]\n",
    "for i in range(len(wiki_info )):\n",
    "    if re.search(r'\\d+', wiki_info[i]):\n",
    "        publications.append(wiki_info[i])\n",
    "    else:\n",
    "        languajes.append(wiki_info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Languaje</th>\n",
       "      <th>Publications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5 892 000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1 159 000+ 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Español</td>\n",
       "      <td>1 532 000+ artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2 323 000+ Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1 556 000+ статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2 123 000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1 541 000+ voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1 065 000+ 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1 010 000+ artigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1 346 000+ haseł</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Languaje          Publications\n",
       "0    English   5 892 000+ articles\n",
       "1        日本語         1 159 000+ 記事\n",
       "2    Español  1 532 000+ artículos\n",
       "3    Deutsch    2 323 000+ Artikel\n",
       "4    Русский     1 556 000+ статей\n",
       "5   Français   2 123 000+ articles\n",
       "6   Italiano       1 541 000+ voci\n",
       "7         中文         1 065 000+ 條目\n",
       "8  Português    1 010 000+ artigos\n",
       "9     Polski      1 346 000+ haseł"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languajes = pd.Series(languajes)\n",
    "publications = pd.Series(publications)\n",
    "lan_pub = pd.concat([languajes, publications], axis=1)\n",
    "lan_pub.columns = ['Languaje', 'Publications']\n",
    "lan_pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "uk_url = 'https://data.gov.uk/'\n",
    "uk_link = requests.get(uk_url).content\n",
    "uk_soup = BeautifulSoup(uk_link, 'html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "uk_table = uk_soup.select('.grid-row a')\n",
    "uk_info = [item.text for item in uk_table]\n",
    "uk_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "lang_url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "lang_link = requests.get(lang_url).content\n",
    "lang_soup = BeautifulSoup(lang_link, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese (macrolanguage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandarin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arabic (macrolanguage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Top Languages\n",
       "0  Chinese (macrolanguage)\n",
       "1                 Mandarin\n",
       "2                  Spanish\n",
       "3                  English\n",
       "4                    Hindi\n",
       "5   Arabic (macrolanguage)\n",
       "6                  Bengali\n",
       "7               Portuguese\n",
       "8                  Russian\n",
       "9                 Japanese"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "lang_lst = lang_soup.select('tbody tr td:nth-child(3)')\n",
    "lang_lst = [lang.text for lang in lang_lst]\n",
    "lang_lst = (lang_lst[0:10])\n",
    "lang_lst = pd.DataFrame([a.strip('\\n') for a in lang_lst])\n",
    "lang_lst.columns = ['Top Languages']\n",
    "lang_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
